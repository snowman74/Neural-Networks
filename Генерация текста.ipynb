{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Домашнее задание № 26.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snowman74/Neural-Networks/blob/main/%D0%93%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D1%8F%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j1Wpkvc3Q2s"
      },
      "source": [
        "from google.colab import files # модуль для загрузки файлов в colab\n",
        "import numpy as np #библиотека для работы с массивами данных\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model # из кераса подгружаем абстрактный класс базовой модели, метод загрузки предобученной модели\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input # из кераса загружаем необходимые слои для нейросети\n",
        "from tensorflow.keras.optimizers import RMSprop, Adadelta # из кераса загружаем выбранный оптимизатор\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # загружаем метод ограничения последовательности заданной длиной\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # загружаем токенизатор кераса для обработки текста\n",
        "from tensorflow.keras import utils # загружаем утилиты кераса для one hot кодировки\n",
        "from tensorflow.keras.utils import plot_model # удобный график для визуализации архитектуры модели\n",
        "\n",
        "import yaml # импортируем модуль для удобной работы с файлами"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "200dSPOYZE7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a069f59-8834-4357-c209-288eddbe0c08"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEA8TR_oerov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4414d67e-76f7-4936-a501-0f3d702da46e"
      },
      "source": [
        "######################\n",
        "# Открываем файл с диалогами\n",
        "######################\n",
        "corpus = open('/content/drive/My Drive/Базы/Диалоги(рассказы).yml', 'r') # открываем файл с диалогами в режиме чтения\n",
        "document = yaml.safe_load(corpus) # загружаем файл *глоссарий\n",
        "conversations = document['разговоры'] # загружаем диалоги из файла и заносим в conversations \n",
        "print('Количество пар вопрос-ответ : {}'.format(len(conversations)))\n",
        "print('Пример диалога : {}'.format(conversations[123]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество пар вопрос-ответ : 11905\n",
            "Пример диалога : ['Перезалил?', 'Да вроде бы нет...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYg8z8Vj76bu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff176dbf-244e-4b1f-8f76-13c091949b1a"
      },
      "source": [
        "######################\n",
        "# Разбираем вопросы-ответы с проставлением тегов ответам\n",
        "######################\n",
        "# Собираем вопросы и ответы в списки\n",
        "questions = list() # здесь будет список вопросов\n",
        "answers = list() # здесь будет список ответов\n",
        "\n",
        "# В каждом диалоге берем фразу и добавляем в лист\n",
        "# Если в ответе не одна фраза - то сцепляем сколько есть\n",
        "for con in conversations: # для каждой пары вопрос-ответ\n",
        "  if len(con) > 2 : # если ответ содержит более двух предложений (кол-во реплик, кол-во вариантов ответа)\n",
        "    questions.append(con[0]) # то вопросительную реплику отправляем в список вопросов\n",
        "    replies = con[1:] # а ответную составляем из последующих строк\n",
        "    ans = '' # здесь соберем ответ\n",
        "    for rep in replies: # каждую реплику в ответной реплике\n",
        "      ans += ' ' + rep \n",
        "    answers.append(ans) #добавим в список ответов\n",
        "  elif len(con)> 1: # если на 1 вопрос приходится 1 ответ\n",
        "    questions.append(con[0]) # то вопросительную реплику отправляем в список вопросов\n",
        "    answers.append(con[1]) # а ответную в список ответов\n",
        "\n",
        "# Очищаем строки с неопределенным типов ответов\n",
        "answersCleaned = list()\n",
        "for i in range(len(answers)):\n",
        "  if type(answers[i]) == str:\n",
        "    answersCleaned.append(answers[i]) #если тип - строка, то добавляем в ответы\n",
        "  else:\n",
        "    questions.pop(i) # если не строка, то ответ не добавился, и плюс убираем соответствующий вопрос\n",
        "\n",
        "# Сделаем теги-метки для начала и конца ответов\n",
        "answers = list()\n",
        "for i in range(len(answersCleaned)):\n",
        "  answers.append( '<START> ' + answersCleaned[i] + ' <END>' )\n",
        "\n",
        "# Выведем обновленные данные на экран\n",
        "print('Вопрос : {}'.format(questions[200]))\n",
        "print('Ответ : {}'.format(answers[200]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Вопрос : Около сотни...\n",
            "Ответ : <START> Точнее! <END>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwmv7JXmRET4"
      },
      "source": [
        "Создадим и обучим чат-бота.\n",
        "Используя три любых простых вопроса, сравним ответы сети на них на разной степени натренированности:\n",
        "\n",
        "a) 20 эпох – удается ли боту отвечать целыми словами?\n",
        "\n",
        "b) + 30 эпох на этой же сетке и с этими же вопросами – появился ли прогресс в качестве\n",
        "ответа сети(ответ целыми предложениями разумной длины)?\n",
        "\n",
        "c) Ещё + 50 эпох – удается ли сети выдавать ответы, “похожие на правду”?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvn1jvRd9tep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26e10863-2c88-473e-cd3f-d7f29292e8de"
      },
      "source": [
        "######################\n",
        "# Подключаем керасовский токенизатор и собираем словарь индексов\n",
        "######################\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# tokenizer = Tokenizer(oov_token='unknown')\n",
        "tokenizer.fit_on_texts(questions + answers) # загружаем в токенизатор список вопросов-ответов для сборки словаря частотности\n",
        "vocabularyItems = list(tokenizer.word_index.items()) # список с cодержимым словаря\n",
        "vocabularySize = len(vocabularyItems)+1 # размер словаря\n",
        "print( 'Фрагмент словаря : {}'.format(vocabularyItems[:50]))\n",
        "print( 'Размер словаря : {}'.format(vocabularySize))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Фрагмент словаря : [('unknown', 1), ('start', 2), ('end', 3), ('что', 4), ('не', 5), ('я', 6), ('а', 7), ('ты', 8), ('это', 9), ('да', 10), ('в', 11), ('нет', 12), ('как', 13), ('и', 14), ('вы', 15), ('ну', 16), ('с', 17), ('на', 18), ('же', 19), ('так', 20), ('он', 21), ('у', 22), ('кто', 23), ('где', 24), ('все', 25), ('мы', 26), ('то', 27), ('мне', 28), ('тебя', 29), ('меня', 30), ('здесь', 31), ('еще', 32), ('почему', 33), ('о', 34), ('там', 35), ('тебе', 36), ('есть', 37), ('его', 38), ('за', 39), ('куда', 40), ('вот', 41), ('ничего', 42), ('вас', 43), ('знаю', 44), ('чем', 45), ('но', 46), ('она', 47), ('они', 48), ('ли', 49), ('чего', 50)]\n",
            "Размер словаря : 15105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-0-rTfzQ2ia"
      },
      "source": [
        "Подготовка выборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4nNBJUQgebF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878f8095-72b8-43dd-8fe6-c3ae413e1233"
      },
      "source": [
        "######################\n",
        "# Устанавливаем закодированные входные данные(вопросы)\n",
        "######################\n",
        "tokenizedQuestions = tokenizer.texts_to_sequences(questions) # разбиваем текст вопросов на последовательности индексов\n",
        "maxLenQuestions = max([ len(x) for x in tokenizedQuestions]) # уточняем длину самого большого вопроса\n",
        "# Делаем последовательности одной длины, заполняя нулями более короткие вопросы\n",
        "paddedQuestions = pad_sequences(tokenizedQuestions, maxlen=maxLenQuestions, padding='post')\n",
        "\n",
        "# Предподготавливаем данные для входа в сеть\n",
        "encoderForInput = np.array(paddedQuestions) # переводим в numpy массив\n",
        "print('Пример оригинального вопроса на вход : {}'.format(questions[100])) \n",
        "print('Пример кодированного вопроса на вход : {}'.format(encoderForInput[100])) \n",
        "print('Размеры закодированного массива вопросов на вход : {}'.format(encoderForInput.shape)) \n",
        "print('Установленная длина вопросов на вход : {}'.format(maxLenQuestions)) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пример оригинального вопроса на вход : Какая же мораль?\n",
            "Пример кодированного вопроса на вход : [ 171   19 5710    0    0    0    0    0    0    0    0]\n",
            "Размеры закодированного массива вопросов на вход : (11900, 11)\n",
            "Установленная длина вопросов на вход : 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tjvhMuzqFJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981f17a5-287d-4f14-e23a-60fd863f6c4d"
      },
      "source": [
        "######################\n",
        "# Устанавливаем раскодированные входные данные(ответы)\n",
        "######################\n",
        "tokenizedAnswers = tokenizer.texts_to_sequences(answers) # разбиваем текст ответов на последовательности индексов\n",
        "maxLenAnswers = max([len(x) for x in tokenizedAnswers]) # уточняем длину самого большого ответа\n",
        "# Делаем последовательности одной длины, заполняя нулями более короткие ответы\n",
        "paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers, padding='post')\n",
        "\n",
        "# Предподготавливаем данные для входа в сеть\n",
        "decoderForInput = np.array(paddedAnswers) # переводим в numpy массив\n",
        "print('Пример оригинального ответа на вход: {}'.format(answers[14])) \n",
        "print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[14][:30])) \n",
        "print('Размеры раскодированного массива ответов на вход : {}'.format(decoderForInput.shape)) \n",
        "print('Установленная длина ответов на вход : {}'.format(maxLenAnswers)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пример оригинального ответа на вход: <START> Составляющие щербета по-херсонски. <END>\n",
            "Пример раскодированного ответа на вход : [    2 10512 10513    54 10514     3     0     0     0     0     0     0\n",
            "     0]\n",
            "Размеры раскодированного массива ответов на вход : (11900, 13)\n",
            "Установленная длина ответов на вход : 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwsKk9dzNeqI"
      },
      "source": [
        "######################\n",
        "# Раскодированные выходные данные(ответы)\n",
        "######################\n",
        "tokenizedAnswers = tokenizer.texts_to_sequences(answers) # разбиваем текст ответов на последовательности индексов\n",
        "for i in range(len(tokenizedAnswers)) : # для разбитых на последовательности ответов\n",
        "  tokenizedAnswers[i] = tokenizedAnswers[i][1:] # избавляемся от тега <START>\n",
        "# Делаем последовательности одной длины, заполняя нулями более короткие ответы\n",
        "paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers , padding='post')\n",
        "\n",
        "oneHotAnswers = utils.to_categorical(paddedAnswers, vocabularySize) # переводим в one hot vector\n",
        "decoderForOutput = np.array(oneHotAnswers) # и сохраняем в виде массива numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRl1k7SVaA6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd90cdd8-883b-43cf-b7d4-a029d71d79f5"
      },
      "source": [
        "print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[100][:21]))  \n",
        "print('Пример раскодированного ответа на выход : {}'.format(decoderForOutput[100][1][:21])) \n",
        "print('Размеры раскодированного массива ответов на выход : {}'.format(decoderForOutput.shape))\n",
        "print('Установленная длина вопросов на выход : {}'.format(maxLenAnswers)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пример раскодированного ответа на вход : [    2   674    20    94 10559     3     0     0     0     0     0     0\n",
            "     0]\n",
            "Пример раскодированного ответа на выход : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "Размеры раскодированного массива ответов на выход : (11900, 13, 15105)\n",
            "Установленная длина вопросов на выход : 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rRKDr4rhXcZ"
      },
      "source": [
        "######################\n",
        "# Первый входной слой, кодер, выходной слой\n",
        "######################\n",
        "encoderInputs = Input(shape=(None , )) # размеры на входе сетки (здесь будет encoderForInput)\n",
        "# Эти данные проходят через слой Embedding (длина словаря, размерность) \n",
        "encoderEmbedding = Embedding(vocabularySize, 200 , mask_zero=True) (encoderInputs)\n",
        "# Затем выход с Embedding пойдёт в LSTM слой, на выходе у которого будет два вектора состояния - state_h , state_c\n",
        "# Вектора состояния - state_h , state_c зададутся в LSTM слое декодера в блоке ниже\n",
        "encoderOutputs, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)\n",
        "encoderStates = [state_h, state_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_yv8Y6QWX2D"
      },
      "source": [
        "######################\n",
        "# Второй входной слой, декодер, выходной слой\n",
        "######################\n",
        "decoderInputs = Input(shape=(None, )) # размеры на входе сетки (здесь будет decoderForInput)\n",
        "# Эти данные проходят через слой Embedding (длина словаря, размерность) \n",
        "# mask_zero=True - игнорировать нулевые padding при передаче в LSTM. Предотвратит вывод ответа типа: \"У меня все хорошо PAD PAD PAD PAD PAD PAD..\"\n",
        "decoderEmbedding = Embedding(vocabularySize, 200, mask_zero=True) (decoderInputs) \n",
        "# Затем выход с Embedding пойдёт в LSTM слой, которому передаются вектора состояния - state_h , state_c\n",
        "decoderLSTM = LSTM(200, return_state=True, return_sequences=True)\n",
        "decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)\n",
        "# И от LSTM'а сигнал decoderOutputs пропускаем через полносвязный слой с софтмаксом на выходе\n",
        "decoderDense = Dense(vocabularySize, activation='softmax') \n",
        "output = decoderDense (decoderOutputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYnTen_UWc5F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "outputId": "e806a8a8-821e-4b3b-fba1-fcc7b821bb48"
      },
      "source": [
        "######################\n",
        "# Собираем тренировочную модель нейросети\n",
        "######################\n",
        "model = Model([encoderInputs, decoderInputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "print(model.summary()) # выведем на экран информацию о построенной модели нейросети\n",
        "plot_model(model, to_file='model.png') # и построим график для визуализации слоев и связей между ними"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 200)    3021000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 200)    3021000     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 15105)  3036105     lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 9,719,705\n",
            "Trainable params: 9,719,705\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv9utvcjh2co"
      },
      "source": [
        "######################\n",
        "# Создаем рабочую модель для вывода ответов на запросы пользователя\n",
        "######################\n",
        "def makeInferenceModels():\n",
        "  # Определим модель кодера, на входе далее будут закодированные вопросы(encoderForInputs), на выходе состояния state_h, state_c\n",
        "  encoderModel = Model(encoderInputs, encoderStates) \n",
        "\n",
        "  decoderStateInput_h = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_h\n",
        "  decoderStateInput_c = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_c\n",
        "\n",
        "  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] # возьмем оба inputs вместе и запишем в decoderStatesInputs\n",
        "\n",
        "  # Берём ответы, прошедшие через эмбединг, вместе с состояниями и подаём LSTM cлою\n",
        "  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)\n",
        "  decoderStates = [state_h, state_c] # LSTM даст нам новые состояния\n",
        "  decoderOutputs = decoderDense(decoderOutputs) # и ответы, которые мы пропустим через полносвязный слой с софтмаксом\n",
        "\n",
        "  # Определим модель декодера, на входе далее будут раскодированные ответы (decoderForInputs) и состояния\n",
        "  # на выходе предсказываемый ответ и новые состояния\n",
        "  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)\n",
        "\n",
        "  return encoderModel , decoderModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSSOhZpgh9LI"
      },
      "source": [
        "######################\n",
        "# Создадим функцию, которая преобразует вопрос пользователя в последовательность индексов\n",
        "######################\n",
        "def strToTokens(sentence: str): # функция принимает строку на вход (предложение с вопросом)\n",
        "  words = sentence.lower().split() # приводит предложение к нижнему регистру и разбирает на слова\n",
        "  tokensList = list() # здесь будет последовательность токенов/индексов\n",
        "  for word in words: # для каждого слова в предложении\n",
        "    tokensList.append(tokenizer.word_index[word]) # определяем токенизатором индекс и добавляем в список\n",
        "\n",
        "    # Функция вернёт вопрос в виде последовательности индексов, ограниченной длиной самого длинного вопроса из нашей базы вопросов\n",
        "  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKWjZyx_oQ96"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbelEm0zhadD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4e3854-ede6-4e0d-82b8-c197714cf05b"
      },
      "source": [
        "# Запустим обучение и сохраним модель\n",
        "model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=20) \n",
        "model.save( '/content/drive/My Drive/модели/model_30epochs(rms)_first_20_epochs.h5' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 2.2139\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.9761\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.9275\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.8882\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.8530\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.8192\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.7874\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.7579\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.7281\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.6964\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.6624\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.6284\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.5953\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.5614\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.5288\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.4972\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.4672\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.4358\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 1.4046\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.3740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ0Dxd1eiEid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a5a690-e802-477a-f741-0b975f9c51f2"
      },
      "source": [
        "######################\n",
        "# Устанавливаем окончательные настройки и запускаем модель\n",
        "######################\n",
        "\n",
        "encModel , decModel = makeInferenceModels() # запускаем функцию для построения модели кодера и декодера\n",
        "\n",
        "for _ in range(6): # задаем количество вопросов, и на каждой итерации в этом диапазоне:\n",
        "  # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом\n",
        "  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))\n",
        "  # Создаём пустой массив размером (1, 1)\n",
        "  emptyTargetSeq = np.zeros((1, 1))    \n",
        "  emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса\n",
        "\n",
        "  stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова\n",
        "  decodedTranslation = '' # здесь будет собираться генерируемый ответ\n",
        "  while not stopCondition : # пока не сработало стоп-условие\n",
        "    # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.\n",
        "    # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния\n",
        "    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)\n",
        "    \n",
        "    #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве\n",
        "    sampledWordIndex = np.argmax( decOutputs[0, 0, :]) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.\n",
        "    sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык\n",
        "    for word , index in tokenizer.word_index.items():\n",
        "      if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря\n",
        "        decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ \n",
        "        sampledWord = word # выбранное слово фиксируем в переменную sampledWord\n",
        "    \n",
        "    # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа\n",
        "    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:\n",
        "      stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию\n",
        "\n",
        "    emptyTargetSeq = np.zeros((1, 1)) # создаем пустой массив\n",
        "    emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова\n",
        "    statesValues = [h, c] # и состояния, обновленные декодером\n",
        "    # и продолжаем цикл с обновленными параметрами\n",
        "  \n",
        "  print(decodedTranslation[:-3]) # выводим ответ сгенерированный декодером"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Задайте вопрос : сколько тебе лет\n",
            " давай \n",
            "Задайте вопрос : как тебя зовут\n",
            " я \n",
            "Задайте вопрос : что думаешь\n",
            " ничего \n",
            "Задайте вопрос : знаешь что\n",
            " нет \n",
            "Задайте вопрос : когда вернешься\n",
            " не знаю \n",
            "Задайте вопрос : голоден\n",
            " ну да \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nD0A2ZVSsJP"
      },
      "source": [
        "Вопросы:\n",
        "\n",
        "1 сколько тебе лет?\n",
        "\n",
        "2 как тебя зовут?\n",
        "\n",
        "3 что думаешь?\n",
        "\n",
        "4 знаешь что?\n",
        "\n",
        "5 когда вернешься?\n",
        "\n",
        "6 голоден?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4ch5begTXGq"
      },
      "source": [
        "Обучим ещё на 30 эпохах"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTqgdKwKSbAv",
        "outputId": "2fecae09-9a8c-4230-c082-2e5ad05e50c0"
      },
      "source": [
        "model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=30) \n",
        "model.save( '/content/drive/My Drive/модели/model_30epochs(rms)_second_30_epochs.h5' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.3435\n",
            "Epoch 2/30\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.3126\n",
            "Epoch 3/30\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.2824\n",
            "Epoch 4/30\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.2537\n",
            "Epoch 5/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.2254\n",
            "Epoch 6/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.1984\n",
            "Epoch 7/30\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.1684\n",
            "Epoch 8/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.1425\n",
            "Epoch 9/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.1178\n",
            "Epoch 10/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.0953\n",
            "Epoch 11/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.0719\n",
            "Epoch 12/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.0516\n",
            "Epoch 13/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.0311\n",
            "Epoch 14/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.0131\n",
            "Epoch 15/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.9955\n",
            "Epoch 16/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.9810\n",
            "Epoch 17/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.9675\n",
            "Epoch 18/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.9547\n",
            "Epoch 19/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.9441\n",
            "Epoch 20/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.9361\n",
            "Epoch 21/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.9299\n",
            "Epoch 22/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.9237\n",
            "Epoch 23/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.9184\n",
            "Epoch 24/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.9138\n",
            "Epoch 25/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.9085\n",
            "Epoch 26/30\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 0.9034\n",
            "Epoch 27/30\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 0.8986\n",
            "Epoch 28/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8943\n",
            "Epoch 29/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8893\n",
            "Epoch 30/30\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lNh6kYfTFCG",
        "outputId": "bc47db07-f6df-481e-99af-da0035730638"
      },
      "source": [
        "######################\n",
        "# Устанавливаем окончательные настройки и запускаем модель\n",
        "######################\n",
        "\n",
        "encModel , decModel = makeInferenceModels() # запускаем функцию для построения модели кодера и декодера\n",
        "\n",
        "for _ in range(6): # задаем количество вопросов, и на каждой итерации в этом диапазоне:\n",
        "  # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом\n",
        "  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))\n",
        "  # Создаём пустой массив размером (1, 1)\n",
        "  emptyTargetSeq = np.zeros((1, 1))    \n",
        "  emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса\n",
        "\n",
        "  stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова\n",
        "  decodedTranslation = '' # здесь будет собираться генерируемый ответ\n",
        "  while not stopCondition : # пока не сработало стоп-условие\n",
        "    # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.\n",
        "    # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния\n",
        "    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)\n",
        "    \n",
        "    #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве\n",
        "    sampledWordIndex = np.argmax( decOutputs[0, 0, :]) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.\n",
        "    sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык\n",
        "    for word , index in tokenizer.word_index.items():\n",
        "      if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря\n",
        "        decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ \n",
        "        sampledWord = word # выбранное слово фиксируем в переменную sampledWord\n",
        "    \n",
        "    # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа\n",
        "    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:\n",
        "      stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию\n",
        "\n",
        "    emptyTargetSeq = np.zeros((1, 1)) # создаем пустой массив\n",
        "    emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова\n",
        "    statesValues = [h, c] # и состояния, обновленные декодером\n",
        "    # и продолжаем цикл с обновленными параметрами\n",
        "  \n",
        "  print(decodedTranslation[:-3]) # выводим ответ сгенерированный декодером"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Задайте вопрос : сколько тебе лет\n",
            " не знаю а сколько нужно \n",
            "Задайте вопрос : как тебя зовут\n",
            " меня \n",
            "Задайте вопрос : что думаешь\n",
            " ничего не надо \n",
            "Задайте вопрос : знаешь что\n",
            " а мама \n",
            "Задайте вопрос : когда вернешься\n",
            " не знаю \n",
            "Задайте вопрос : голоден\n",
            " и чего \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSaldc75We8b"
      },
      "source": [
        "и ещё + 50 эпох"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvHLaCOZWa68",
        "outputId": "0874111d-2b45-4181-9c31-2dfcc4056dcc"
      },
      "source": [
        "model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=50) \n",
        "model.save( '/content/drive/My Drive/модели/model_30epochs(rms)_third_50_epochs.h5' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 0.8799\n",
            "Epoch 2/50\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 0.8757\n",
            "Epoch 3/50\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 0.8703\n",
            "Epoch 4/50\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 0.8655\n",
            "Epoch 5/50\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 0.8603\n",
            "Epoch 6/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8553\n",
            "Epoch 7/50\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 0.8525\n",
            "Epoch 8/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8494\n",
            "Epoch 9/50\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 0.8464\n",
            "Epoch 10/50\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 0.8433\n",
            "Epoch 11/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8404\n",
            "Epoch 12/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8374\n",
            "Epoch 13/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8340\n",
            "Epoch 14/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8303\n",
            "Epoch 15/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8276\n",
            "Epoch 16/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8244\n",
            "Epoch 17/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.8217\n",
            "Epoch 18/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.8186\n",
            "Epoch 19/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.8155\n",
            "Epoch 20/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8130\n",
            "Epoch 21/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.8100\n",
            "Epoch 22/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.8071\n",
            "Epoch 23/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.8044\n",
            "Epoch 24/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.8016\n",
            "Epoch 25/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7996\n",
            "Epoch 26/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7973\n",
            "Epoch 27/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7950\n",
            "Epoch 28/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7930\n",
            "Epoch 29/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7912\n",
            "Epoch 30/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7888\n",
            "Epoch 31/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7870\n",
            "Epoch 32/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7849\n",
            "Epoch 33/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7822\n",
            "Epoch 34/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7812\n",
            "Epoch 35/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7786\n",
            "Epoch 36/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7777\n",
            "Epoch 37/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.7754\n",
            "Epoch 38/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7735\n",
            "Epoch 39/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7719\n",
            "Epoch 40/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.7704\n",
            "Epoch 41/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.7690\n",
            "Epoch 42/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7670\n",
            "Epoch 43/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7658\n",
            "Epoch 44/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7640\n",
            "Epoch 45/50\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 0.7623\n",
            "Epoch 46/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7613\n",
            "Epoch 47/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7602\n",
            "Epoch 48/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7579\n",
            "Epoch 49/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7566\n",
            "Epoch 50/50\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 0.7550\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dgmwl6kWa6-",
        "outputId": "37d04244-7ad9-48fd-f078-36d7926a180e"
      },
      "source": [
        "######################\n",
        "# Устанавливаем окончательные настройки и запускаем модель\n",
        "######################\n",
        "\n",
        "encModel , decModel = makeInferenceModels() # запускаем функцию для построения модели кодера и декодера\n",
        "\n",
        "for _ in range(6): # задаем количество вопросов, и на каждой итерации в этом диапазоне:\n",
        "  # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом\n",
        "  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))\n",
        "  # Создаём пустой массив размером (1, 1)\n",
        "  emptyTargetSeq = np.zeros((1, 1))    \n",
        "  emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса\n",
        "\n",
        "  stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова\n",
        "  decodedTranslation = '' # здесь будет собираться генерируемый ответ\n",
        "  while not stopCondition : # пока не сработало стоп-условие\n",
        "    # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.\n",
        "    # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния\n",
        "    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)\n",
        "    \n",
        "    #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве\n",
        "    sampledWordIndex = np.argmax( decOutputs[0, 0, :]) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.\n",
        "    sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык\n",
        "    for word , index in tokenizer.word_index.items():\n",
        "      if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря\n",
        "        decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ \n",
        "        sampledWord = word # выбранное слово фиксируем в переменную sampledWord\n",
        "    \n",
        "    # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа\n",
        "    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:\n",
        "      stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию\n",
        "\n",
        "    emptyTargetSeq = np.zeros((1, 1)) # создаем пустой массив\n",
        "    emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова\n",
        "    statesValues = [h, c] # и состояния, обновленные декодером\n",
        "    # и продолжаем цикл с обновленными параметрами\n",
        "  \n",
        "  print(decodedTranslation[:-3]) # выводим ответ сгенерированный декодером"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Задайте вопрос : сколько тебе лет\n",
            " не знаю а сколько нужно \n",
            "Задайте вопрос : как тебя зовут\n",
            " меня \n",
            "Задайте вопрос : что думаешь\n",
            " ничего \n",
            "Задайте вопрос : знаешь что\n",
            " а мама \n",
            "Задайте вопрос : когда вернешься\n",
            " сейчас \n",
            "Задайте вопрос : голоден\n",
            " и чего же он мне не чего в мне так \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEeKhr6Net2S"
      },
      "source": [
        "# ВЫВОДЫ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRQsWKPjewB_"
      },
      "source": [
        "**После первых 20 эпох ответы были однословными и почти не попадали под понятие \"логичные\"**\n",
        "\n",
        "Задайте вопрос : сколько тебе лет\n",
        "\n",
        " давай \n",
        "\n",
        "Задайте вопрос : как тебя зовут\n",
        "\n",
        " я \n",
        "\n",
        "Задайте вопрос : что думаешь\n",
        "\n",
        " ничего \n",
        "\n",
        "Задайте вопрос : знаешь что\n",
        "\n",
        " нет \n",
        "\n",
        "Задайте вопрос : когда вернешься\n",
        "\n",
        " не знаю \n",
        "\n",
        "Задайте вопрос : голоден\n",
        "\n",
        " ну да\n",
        "\n",
        "\n",
        "**После ещё 30 эпох ответы стали намного логичнее, однако (и скорее всего из-за моих не совсем понятных вопросов) некоторые ответы всё равно оказались не в тему.**\n",
        "\n",
        "Задайте вопрос : сколько тебе лет\n",
        "\n",
        " не знаю а сколько нужно \n",
        "\n",
        "Задайте вопрос : как тебя зовут\n",
        "\n",
        " меня \n",
        "\n",
        "Задайте вопрос : что думаешь\n",
        "\n",
        " ничего не надо \n",
        "\n",
        "Задайте вопрос : знаешь что\n",
        "\n",
        " а мама \n",
        "\n",
        "Задайте вопрос : когда вернешься\n",
        "\n",
        " не знаю \n",
        "\n",
        "Задайте вопрос : голоден\n",
        " \n",
        " и чего\n",
        "\n",
        "Ещё после 50 эпох разница есть, например на вопрос *КОГДА ВЕРНЕШЬСЯ?* вместо *НЕ ЗНАЮ*, сеть ответила *СЕЙЧАС*, что, как мне кажется более логично, но вот на вопрос *ГОЛОДЕН* выдает  почти не связные слова.\n",
        "\n",
        "\n",
        "Задайте вопрос : сколько тебе лет\n",
        " не знаю а сколько нужно \n",
        "Задайте вопрос : как тебя зовут\n",
        " меня \n",
        "Задайте вопрос : что думаешь\n",
        " ничего \n",
        "Задайте вопрос : знаешь что\n",
        " а мама \n",
        "Задайте вопрос : когда вернешься\n",
        " сейчас \n",
        "Задайте вопрос : голоден\n",
        " и чего же он мне не чего в мне так "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH4N70IqTE9P",
        "outputId": "d00827d8-898e-4002-89d9-fb8921e3e311"
      },
      "source": [
        "######################\n",
        "# Устанавливаем окончательные настройки и запускаем модель\n",
        "######################\n",
        "\n",
        "encModel , decModel = makeInferenceModels() # запускаем функцию для построения модели кодера и декодера\n",
        "\n",
        "for _ in range(20): # задаем количество вопросов, и на каждой итерации в этом диапазоне:\n",
        "  # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом\n",
        "  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))\n",
        "  # Создаём пустой массив размером (1, 1)\n",
        "  emptyTargetSeq = np.zeros((1, 1))    \n",
        "  emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса\n",
        "\n",
        "  stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова\n",
        "  decodedTranslation = '' # здесь будет собираться генерируемый ответ\n",
        "  while not stopCondition : # пока не сработало стоп-условие\n",
        "    # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.\n",
        "    # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния\n",
        "    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)\n",
        "    \n",
        "    #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве\n",
        "    sampledWordIndex = np.argmax( decOutputs[0, 0, :]) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.\n",
        "    sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык\n",
        "    for word , index in tokenizer.word_index.items():\n",
        "      if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря\n",
        "        decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ \n",
        "        sampledWord = word # выбранное слово фиксируем в переменную sampledWord\n",
        "    \n",
        "    # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа\n",
        "    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:\n",
        "      stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию\n",
        "\n",
        "    emptyTargetSeq = np.zeros((1, 1)) # создаем пустой массив\n",
        "    emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова\n",
        "    statesValues = [h, c] # и состояния, обновленные декодером\n",
        "    # и продолжаем цикл с обновленными параметрами\n",
        "  \n",
        "  print(decodedTranslation[:-3]) # выводим ответ сгенерированный декодером"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Задайте вопрос : который год\n",
            " да \n",
            "Задайте вопрос : сколько времени\n",
            " не знаю \n",
            "Задайте вопрос : ты кто\n",
            " я я \n",
            "Задайте вопрос : как твои дела\n",
            " все в порядке \n",
            "Задайте вопрос : чем занимаешься\n",
            " да я что сейчас вы теперь видели \n",
            "Задайте вопрос : где работаешь\n",
            " еще ну на \n",
            "Задайте вопрос : умеешь танцевать\n",
            " как же что это \n",
            "Задайте вопрос : любишь чай\n",
            " как это я нет \n",
            "Задайте вопрос : давай дружить\n",
            " ты это хорошо вот и да \n",
            "Задайте вопрос : когда война\n",
            " думаю да \n",
            "Задайте вопрос : куришь\n",
            " нет \n",
            "Задайте вопрос : учился в школе\n",
            " ничего \n",
            "Задайте вопрос : на каком языке говоришь\n",
            " у меня на тебе а вот и он \n",
            "Задайте вопрос : что такое хлеб\n",
            " на что то где мне ну \n",
            "Задайте вопрос : который час\n",
            " я бы тоже не могу \n",
            "Задайте вопрос : который час\n",
            " я бы тоже не могу \n",
            "Задайте вопрос : давай поговорим\n",
            " нет \n",
            "Задайте вопрос : почему\n",
            " не знаю хочешь я \n",
            "Задайте вопрос : ну и ладно\n",
            " а что \n",
            "Задайте вопрос : а ничего хорошего\n",
            " ну так он вообще сейчас \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-150TNDKSa-y"
      },
      "source": [
        "Tokenizer?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjcpEP6diOl4"
      },
      "source": [
        "# Добьемся максимально низкого Loss’а в обучении модели\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTtHG3ci9izZ"
      },
      "source": [
        "######################\n",
        "# Подключаем керасовский токенизатор и собираем словарь индексов\n",
        "######################\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# tokenizer = Tokenizer(oov_token='unknown')\n",
        "tokenizer.fit_on_texts(questions + answers) # загружаем в токенизатор список вопросов-ответов для сборки словаря частотности\n",
        "vocabularyItems = list(tokenizer.word_index.items()) # список с cодержимым словаря\n",
        "vocabularySize = len(vocabularyItems)+1 # размер словаря\n",
        "\n",
        "######################\n",
        "# Устанавливаем закодированные входные данные(вопросы)\n",
        "######################\n",
        "tokenizedQuestions = tokenizer.texts_to_sequences(questions) # разбиваем текст вопросов на последовательности индексов\n",
        "maxLenQuestions = max([ len(x) for x in tokenizedQuestions]) # уточняем длину самого большого вопроса\n",
        "# Делаем последовательности одной длины, заполняя нулями более короткие вопросы\n",
        "paddedQuestions = pad_sequences(tokenizedQuestions, maxlen=maxLenQuestions, padding='post')\n",
        "\n",
        "# Предподготавливаем данные для входа в сеть\n",
        "encoderForInput = np.array(paddedQuestions) # переводим в numpy массив\n",
        "######################\n",
        "# Устанавливаем раскодированные входные данные(ответы)\n",
        "######################\n",
        "tokenizedAnswers = tokenizer.texts_to_sequences(answers) # разбиваем текст ответов на последовательности индексов\n",
        "maxLenAnswers = max([len(x) for x in tokenizedAnswers]) # уточняем длину самого большого ответа\n",
        "# Делаем последовательности одной длины, заполняя нулями более короткие ответы\n",
        "paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers, padding='post')\n",
        "\n",
        "# Предподготавливаем данные для входа в сеть\n",
        "decoderForInput = np.array(paddedAnswers) # переводим в numpy массив\n",
        "######################\n",
        "# Раскодированные выходные данные(ответы)\n",
        "######################\n",
        "tokenizedAnswers = tokenizer.texts_to_sequences(answers) # разбиваем текст ответов на последовательности индексов\n",
        "for i in range(len(tokenizedAnswers)) : # для разбитых на последовательности ответов\n",
        "  tokenizedAnswers[i] = tokenizedAnswers[i][1:] # избавляемся от тега <START>\n",
        "# Делаем последовательности одной длины, заполняя нулями более короткие ответы\n",
        "paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers , padding='post')\n",
        "\n",
        "oneHotAnswers = utils.to_categorical(paddedAnswers, vocabularySize) # переводим в one hot vector\n",
        "decoderForOutput = np.array(oneHotAnswers) # и сохраняем в виде массива numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebXnVSqq89NN"
      },
      "source": [
        "######################\n",
        "# Создаем рабочую модель для вывода ответов на запросы пользователя\n",
        "######################\n",
        "def makeInferenceModels():\n",
        "  # Определим модель кодера, на входе далее будут закодированные вопросы(encoderForInputs), на выходе состояния state_h, state_c\n",
        "  encoderModel = Model(encoderInputs, encoderStates) \n",
        "\n",
        "  decoderStateInput_h = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_h\n",
        "  decoderStateInput_c = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_c\n",
        "\n",
        "  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] # возьмем оба inputs вместе и запишем в decoderStatesInputs\n",
        "\n",
        "  # Берём ответы, прошедшие через эмбединг, вместе с состояниями и подаём LSTM cлою\n",
        "  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)\n",
        "  decoderStates = [state_h, state_c] # LSTM даст нам новые состояния\n",
        "  decoderOutputs = decoderDense(decoderOutputs) # и ответы, которые мы пропустим через полносвязный слой с софтмаксом\n",
        "\n",
        "  # Определим модель декодера, на входе далее будут раскодированные ответы (decoderForInputs) и состояния\n",
        "  # на выходе предсказываемый ответ и новые состояния\n",
        "  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)\n",
        "\n",
        "  return encoderModel , decoderModel\n",
        "\n",
        "######################\n",
        "# Создадим функцию, которая преобразует вопрос пользователя в последовательность индексов\n",
        "######################\n",
        "def strToTokens(sentence: str): # функция принимает строку на вход (предложение с вопросом)\n",
        "  words = sentence.lower().split() # приводит предложение к нижнему регистру и разбирает на слова\n",
        "  tokensList = list() # здесь будет последовательность токенов/индексов\n",
        "  for word in words: # для каждого слова в предложении\n",
        "    tokensList.append(tokenizer.word_index[word]) # определяем токенизатором индекс и добавляем в список\n",
        "\n",
        "    # Функция вернёт вопрос в виде последовательности индексов, ограниченной длиной самого длинного вопроса из нашей базы вопросов\n",
        "  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS-Jic7d9IT3"
      },
      "source": [
        "######################\n",
        "# Первый входной слой, кодер, выходной слой\n",
        "######################\n",
        "def make_net(num_filters=200,):\n",
        "  encoderInputs = Input(shape=(None , )) # размеры на входе сетки (здесь будет encoderForInput)\n",
        "  # Эти данные проходят через слой Embedding (длина словаря, размерность) \n",
        "  encoderEmbedding = Embedding(vocabularySize, num_filters , mask_zero=True) (encoderInputs)\n",
        "  # Затем выход с Embedding пойдёт в LSTM слой, на выходе у которого будет два вектора состояния - state_h , state_c\n",
        "  # Вектора состояния - state_h , state_c зададутся в LSTM слое декодера в блоке ниже\n",
        "  encoderOutputs, state_h , state_c = LSTM(num_filters, return_state=True)(encoderEmbedding)\n",
        "  encoderStates = [state_h, state_c]\n",
        "\n",
        "\n",
        "  ######################\n",
        "  # Второй входной слой, декодер, выходной слой\n",
        "  ######################\n",
        "  decoderInputs = Input(shape=(None, )) # размеры на входе сетки (здесь будет decoderForInput)\n",
        "  # Эти данные проходят через слой Embedding (длина словаря, размерность) \n",
        "  # mask_zero=True - игнорировать нулевые padding при передаче в LSTM. Предотвратит вывод ответа типа: \"У меня все хорошо PAD PAD PAD PAD PAD PAD..\"\n",
        "  decoderEmbedding = Embedding(vocabularySize, num_filters, mask_zero=True) (decoderInputs) \n",
        "  # Затем выход с Embedding пойдёт в LSTM слой, которому передаются вектора состояния - state_h , state_c\n",
        "  decoderLSTM = LSTM(num_filters, return_state=True, return_sequences=True)\n",
        "  decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)\n",
        "  # И от LSTM'а сигнал decoderOutputs пропускаем через полносвязный слой с софтмаксом на выходе\n",
        "  decoderDense = Dense(vocabularySize, activation='softmax') \n",
        "  output = decoderDense (decoderOutputs)\n",
        "\n",
        "  ######################\n",
        "  # Собираем тренировочную модель нейросети\n",
        "  ######################\n",
        "\n",
        "  model = Model([encoderInputs, decoderInputs], output)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_YT_Ub-9IT3"
      },
      "source": [
        "model = make_net()\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCN2QQLE-int"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxToE21oB7Km"
      },
      "source": [
        "def save_hist(history, PATH):\n",
        "  hist_df = pd.DataFrame(history.history) \n",
        "  hist_df.to_excel(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIEUw55t89NO",
        "outputId": "d1d42608-a91d-4650-8157-ed5d1f3f78ad"
      },
      "source": [
        "# Запустим обучение и сохраним модель\n",
        "history = model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=20) \n",
        "model.save( '/content/drive/My Drive/модели/model_original_HW26.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 2.2115\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.9759\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 10s 43ms/step - loss: 1.9277\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.8896\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.8554\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.8227\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.7916\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 10s 44ms/step - loss: 1.7596\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.7271\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.6933\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.6588\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.6278\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 1.5949\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.5616\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.5292\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 1.4958\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.4634\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 1.4316\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 10s 41ms/step - loss: 1.3999\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 10s 42ms/step - loss: 1.3673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9oMc58fDfjY"
      },
      "source": [
        "save_hist(history, '/content/drive/MyDrive/Домашнее задание №26 истории/hist.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn_MoyG0-LsP"
      },
      "source": [
        "Для начала попробуем поэкспериментировать с пространством Embedding на небольшом количестве эпох, потом сравним с ошибкой эталона из ноутбука"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymsn5KyFA5vJ",
        "outputId": "2a56254f-112c-46f8-c3eb-96b2b49081f3"
      },
      "source": [
        "for num in [220, 240, 260, 280, 300]:\n",
        "  model = make_net(num_filters=num)\n",
        "  model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "  history = model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=20) \n",
        "  model.save( f'/content/drive/My Drive/Домашнее задание №26 истории/model_embd_{num}.h5')\n",
        "  save_hist(history, f'/content/drive/MyDrive/Домашнее задание №26 истории/hist_emb_{num}.xlsx')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 2.2079\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.9744\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.9234\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.8848\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.8495\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.8145\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.7828\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.7494\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.7146\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.6794\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.6456\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.6103\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 11s 44ms/step - loss: 1.5734\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 11s 44ms/step - loss: 1.5392\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 11s 44ms/step - loss: 1.5044\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 10s 44ms/step - loss: 1.4687\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 11s 44ms/step - loss: 1.4322\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 10s 44ms/step - loss: 1.3974\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 10s 44ms/step - loss: 1.3632\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 11s 44ms/step - loss: 1.3319\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 11s 46ms/step - loss: 2.1929\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 11s 46ms/step - loss: 1.9686\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.9168\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.8768\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.8416\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 11s 46ms/step - loss: 1.8067\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.7709\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.7319\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.6925\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.6520\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.6106\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.5720\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.5359\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.4977\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.4573\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.4170\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.3798\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.3447\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.3122\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 11s 45ms/step - loss: 1.2792\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 2.1848\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.9675\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.9144\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.8735\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.8362\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.7988\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 12s 49ms/step - loss: 1.7611\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 12s 49ms/step - loss: 1.7200\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.6785\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.6354\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.5921\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.5494\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.5065\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.4639\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.4207\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.3792\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.3387\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 11s 46ms/step - loss: 1.3003\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 11s 47ms/step - loss: 1.2639\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 11s 46ms/step - loss: 1.2278\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 12s 49ms/step - loss: 2.1835\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 12s 48ms/step - loss: 1.9646\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.9102\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.8681\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.8280\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.7889\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 12s 48ms/step - loss: 1.7478\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.7054\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.6598\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.6160\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.5724\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.5282\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.4834\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.4372\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.3903\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.3447\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.2993\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.2601\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.2213\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 11s 48ms/step - loss: 1.1839\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 2.1755\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.9624\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.9101\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.8691\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.8297\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.7904\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.7482\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.7056\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.6591\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.6110\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.5636\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.5127\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.4651\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.4190\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.3713\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.3265\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.2822\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.2373\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.1960\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.1555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WGzDmLIIuGx"
      },
      "source": [
        "видим, что ошибка падает с увеличением размера Embedding, продолжим эксперимент"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6GPzQpi8n9Y",
        "outputId": "c6ccbe94-f554-45c8-e442-1766345fc1ec"
      },
      "source": [
        "for num in range(300, 500, 20):\n",
        "  model = make_net(num_filters=num)\n",
        "  model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "  history = model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=20) \n",
        "  model.save( f'/content/drive/My Drive/Домашнее задание №26 истории/model_embd_{num}.h5')\n",
        "  save_hist(history, f'/content/drive/MyDrive/Домашнее задание №26 истории/hist_emb_{num}.xlsx')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 2.1723\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.9636\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.9094\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.8649\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.8228\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.7833\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.7391\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.6932\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.6458\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.5973\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.5491\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.5009\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.4525\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.4045\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.3576\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.3124\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.2686\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.2265\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.1849\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 12s 50ms/step - loss: 1.1437\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 2.1718\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.9615\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.9053\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.8610\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.8196\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.7780\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.7332\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.6870\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.6403\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.5932\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 12s 51ms/step - loss: 1.5452\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.4966\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.4496\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.3994\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.3517\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.3065\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.2644\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.2223\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.1804\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 12s 52ms/step - loss: 1.1394\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 13s 54ms/step - loss: 2.1627\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 13s 54ms/step - loss: 1.9586\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.9017\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.8567\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.8156\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.7712\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.7239\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 13s 54ms/step - loss: 1.6749\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.6220\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.5690\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.5132\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 13s 54ms/step - loss: 1.4589\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 13s 54ms/step - loss: 1.4074\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 13s 54ms/step - loss: 1.3540\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.3015\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.2501\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.2031\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.1581\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.1136\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 13s 53ms/step - loss: 1.0706\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 13s 56ms/step - loss: 2.1581\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 13s 57ms/step - loss: 1.9588\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 13s 56ms/step - loss: 1.9028\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 13s 55ms/step - loss: 1.8564\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 13s 56ms/step - loss: 1.8103\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 13s 56ms/step - loss: 1.7641\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 13s 55ms/step - loss: 1.7149\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 13s 55ms/step - loss: 1.6669\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 13s 55ms/step - loss: 1.6128\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 13s 55ms/step - loss: 1.5604\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 13s 56ms/step - loss: 1.5055\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 13s 56ms/step - loss: 1.4488\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 13s 55ms/step - loss: 1.3919\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 13s 56ms/step - loss: 1.3341\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 13s 56ms/step - loss: 1.2786\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 13s 55ms/step - loss: 1.2274\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 13s 55ms/step - loss: 1.1786\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 13s 55ms/step - loss: 1.1306\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 13s 55ms/step - loss: 1.0816\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 13s 55ms/step - loss: 1.0369\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 2.1544\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 14s 57ms/step - loss: 1.9571\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 14s 57ms/step - loss: 1.9013\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.8520\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.8050\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.7576\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.7098\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.6588\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.6026\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.5471\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.4890\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.4320\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.3727\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.3151\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.2570\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.2014\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.1487\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.0954\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 14s 58ms/step - loss: 1.0446\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 14s 57ms/step - loss: 0.9943\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 2.1521\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.9574\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.8971\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.8469\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.7995\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.7488\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.6956\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.6375\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.5759\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.5131\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.4521\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.3915\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.3296\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.2680\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.2088\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.1525\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.1004\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 1.0475\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 0.9983\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 14s 59ms/step - loss: 0.9513\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 15s 61ms/step - loss: 2.1485\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 14s 61ms/step - loss: 1.9542\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 15s 61ms/step - loss: 1.8949\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 14s 61ms/step - loss: 1.8442\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 14s 61ms/step - loss: 1.7970\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 15s 61ms/step - loss: 1.7447\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 14s 61ms/step - loss: 1.6889\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 15s 62ms/step - loss: 1.6292\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 14s 61ms/step - loss: 1.5644\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 15s 61ms/step - loss: 1.4988\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 15s 61ms/step - loss: 1.4335\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 15s 61ms/step - loss: 1.3705\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 14s 60ms/step - loss: 1.3033\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 15s 61ms/step - loss: 1.2401\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 15s 61ms/step - loss: 1.1780\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 14s 60ms/step - loss: 1.1191\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 15s 61ms/step - loss: 1.0624\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 14s 61ms/step - loss: 1.0084\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 15s 61ms/step - loss: 0.9580\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 14s 60ms/step - loss: 0.9116\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 15s 64ms/step - loss: 2.1420\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.9520\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.8920\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.8404\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.7903\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.7366\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.6805\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.6194\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 15s 64ms/step - loss: 1.5552\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.4886\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.4188\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.3561\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.2909\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.2287\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.1656\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.1037\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 1.0469\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 15s 64ms/step - loss: 0.9970\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 0.9468\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 15s 63ms/step - loss: 0.8998\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 16s 65ms/step - loss: 2.1433\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 16s 65ms/step - loss: 1.9515\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 16s 65ms/step - loss: 1.8909\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 15s 65ms/step - loss: 1.8384\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 15s 65ms/step - loss: 1.7866\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 16s 65ms/step - loss: 1.7320\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 16s 65ms/step - loss: 1.6713\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 16s 65ms/step - loss: 1.6067\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 16s 65ms/step - loss: 1.5391\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 15s 65ms/step - loss: 1.4690\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 16s 65ms/step - loss: 1.4008\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 16s 65ms/step - loss: 1.3304\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 15s 65ms/step - loss: 1.2617\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 15s 65ms/step - loss: 1.1956\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 15s 65ms/step - loss: 1.1303\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 16s 65ms/step - loss: 1.0693\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 15s 65ms/step - loss: 1.0085\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 15s 65ms/step - loss: 0.9542\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 15s 65ms/step - loss: 0.9021\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 15s 65ms/step - loss: 0.8550\n",
            "Epoch 1/20\n",
            "238/238 [==============================] - 16s 67ms/step - loss: 2.1394\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.9507\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.8875\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.8341\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.7817\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 16s 67ms/step - loss: 1.7256\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.6631\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 16s 67ms/step - loss: 1.5932\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.5242\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.4527\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.3806\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.3084\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.2368\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.1652\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.0981\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 1.0344\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 0.9745\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 0.9205\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 0.8718\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 16s 66ms/step - loss: 0.8253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XngzH3en8n7S",
        "outputId": "d0cf15b3-97c9-480a-dc17-8e87f3ca7bc2"
      },
      "source": [
        "for opt in [Adam(), Adadelta()]:\n",
        "  for batch in range(50, 200, 10):\n",
        "    model = make_net(num_filters=500)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
        "    history = model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=batch, epochs=20) \n",
        "    model.save( f'/content/drive/My Drive/Домашнее задание №26 истории/model_opt_{opt}_batch_{batch}.h5')\n",
        "    save_hist(history, f'/content/drive/MyDrive/Домашнее задание №26 истории/hist_opt_{opt}_batch_{batch}.xlsx')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "238/238 [==============================] - 22s 94ms/step - loss: 2.2819\n",
            "Epoch 2/20\n",
            "238/238 [==============================] - 23s 95ms/step - loss: 1.9205\n",
            "Epoch 3/20\n",
            "238/238 [==============================] - 23s 95ms/step - loss: 1.8271\n",
            "Epoch 4/20\n",
            "238/238 [==============================] - 23s 95ms/step - loss: 1.7400\n",
            "Epoch 5/20\n",
            "238/238 [==============================] - 23s 95ms/step - loss: 1.6467\n",
            "Epoch 6/20\n",
            "238/238 [==============================] - 23s 95ms/step - loss: 1.5482\n",
            "Epoch 7/20\n",
            "238/238 [==============================] - 23s 97ms/step - loss: 1.4387\n",
            "Epoch 8/20\n",
            "238/238 [==============================] - 23s 97ms/step - loss: 1.3156\n",
            "Epoch 9/20\n",
            "238/238 [==============================] - 22s 94ms/step - loss: 1.1811\n",
            "Epoch 10/20\n",
            "238/238 [==============================] - 22s 94ms/step - loss: 1.0415\n",
            "Epoch 11/20\n",
            "238/238 [==============================] - 23s 95ms/step - loss: 0.8974\n",
            "Epoch 12/20\n",
            "238/238 [==============================] - 22s 94ms/step - loss: 0.7618\n",
            "Epoch 13/20\n",
            "238/238 [==============================] - 23s 98ms/step - loss: 0.6376\n",
            "Epoch 14/20\n",
            "238/238 [==============================] - 23s 95ms/step - loss: 0.5295\n",
            "Epoch 15/20\n",
            "238/238 [==============================] - 23s 95ms/step - loss: 0.4379\n",
            "Epoch 16/20\n",
            "238/238 [==============================] - 23s 96ms/step - loss: 0.3615\n",
            "Epoch 17/20\n",
            "238/238 [==============================] - 22s 94ms/step - loss: 0.2976\n",
            "Epoch 18/20\n",
            "238/238 [==============================] - 23s 95ms/step - loss: 0.2441\n",
            "Epoch 19/20\n",
            "238/238 [==============================] - 23s 95ms/step - loss: 0.1997\n",
            "Epoch 20/20\n",
            "238/238 [==============================] - 23s 96ms/step - loss: 0.1638\n",
            "Epoch 1/20\n",
            "199/199 [==============================] - 21s 106ms/step - loss: 2.1768\n",
            "Epoch 2/20\n",
            "199/199 [==============================] - 20s 103ms/step - loss: 1.8549\n",
            "Epoch 3/20\n",
            "199/199 [==============================] - 20s 103ms/step - loss: 1.7212\n",
            "Epoch 4/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 1.5925\n",
            "Epoch 5/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 1.4531\n",
            "Epoch 6/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 1.3080\n",
            "Epoch 7/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 1.1654\n",
            "Epoch 8/20\n",
            "199/199 [==============================] - 20s 101ms/step - loss: 1.0302\n",
            "Epoch 9/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 0.9088\n",
            "Epoch 10/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 0.8048\n",
            "Epoch 11/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 0.7144\n",
            "Epoch 12/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 0.6357\n",
            "Epoch 13/20\n",
            "199/199 [==============================] - 20s 103ms/step - loss: 0.5636\n",
            "Epoch 14/20\n",
            "199/199 [==============================] - 20s 101ms/step - loss: 0.4972\n",
            "Epoch 15/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 0.4380\n",
            "Epoch 16/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 0.3838\n",
            "Epoch 17/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 0.3342\n",
            "Epoch 18/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 0.2910\n",
            "Epoch 19/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 0.2524\n",
            "Epoch 20/20\n",
            "199/199 [==============================] - 20s 102ms/step - loss: 0.2207\n",
            "Epoch 1/20\n",
            "170/170 [==============================] - 19s 111ms/step - loss: 2.2036\n",
            "Epoch 2/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 1.8738\n",
            "Epoch 3/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 1.7477\n",
            "Epoch 4/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 1.6288\n",
            "Epoch 5/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 1.5075\n",
            "Epoch 6/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 1.3757\n",
            "Epoch 7/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 1.2383\n",
            "Epoch 8/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 1.1025\n",
            "Epoch 9/20\n",
            "170/170 [==============================] - 18s 106ms/step - loss: 0.9762\n",
            "Epoch 10/20\n",
            "170/170 [==============================] - 19s 109ms/step - loss: 0.8645\n",
            "Epoch 11/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 0.7692\n",
            "Epoch 12/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 0.6851\n",
            "Epoch 13/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 0.6103\n",
            "Epoch 14/20\n",
            "170/170 [==============================] - 18s 107ms/step - loss: 0.5410\n",
            "Epoch 15/20\n",
            "170/170 [==============================] - 18s 106ms/step - loss: 0.4762\n",
            "Epoch 16/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 0.4159\n",
            "Epoch 17/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 0.3618\n",
            "Epoch 18/20\n",
            "170/170 [==============================] - 18s 108ms/step - loss: 0.3128\n",
            "Epoch 19/20\n",
            "170/170 [==============================] - 18s 105ms/step - loss: 0.2698\n",
            "Epoch 20/20\n",
            "170/170 [==============================] - 18s 104ms/step - loss: 0.2330\n",
            "Epoch 1/20\n",
            "149/149 [==============================] - 18s 120ms/step - loss: 2.2240\n",
            "Epoch 2/20\n",
            "149/149 [==============================] - 17s 113ms/step - loss: 1.8773\n",
            "Epoch 3/20\n",
            "149/149 [==============================] - 17s 113ms/step - loss: 1.7543\n",
            "Epoch 4/20\n",
            "149/149 [==============================] - 17s 112ms/step - loss: 1.6391\n",
            "Epoch 5/20\n",
            "149/149 [==============================] - 17s 114ms/step - loss: 1.5318\n",
            "Epoch 6/20\n",
            "149/149 [==============================] - 17s 114ms/step - loss: 1.4055\n",
            "Epoch 7/20\n",
            "149/149 [==============================] - 17s 113ms/step - loss: 1.2786\n",
            "Epoch 8/20\n",
            "149/149 [==============================] - 17s 113ms/step - loss: 1.1544\n",
            "Epoch 9/20\n",
            "149/149 [==============================] - 17s 113ms/step - loss: 1.0394\n",
            "Epoch 10/20\n",
            "149/149 [==============================] - 17s 114ms/step - loss: 0.9358\n",
            "Epoch 11/20\n",
            "149/149 [==============================] - 17s 113ms/step - loss: 0.8531\n",
            "Epoch 12/20\n",
            "149/149 [==============================] - 17s 113ms/step - loss: 0.7722\n",
            "Epoch 13/20\n",
            "149/149 [==============================] - 17s 113ms/step - loss: 0.7016\n",
            "Epoch 14/20\n",
            "149/149 [==============================] - 18s 118ms/step - loss: 0.6390\n",
            "Epoch 15/20\n",
            "149/149 [==============================] - 17s 116ms/step - loss: 0.5813\n",
            "Epoch 16/20\n",
            "149/149 [==============================] - 17s 112ms/step - loss: 0.5275\n",
            "Epoch 17/20\n",
            "149/149 [==============================] - 17s 113ms/step - loss: 0.4783\n",
            "Epoch 18/20\n",
            "149/149 [==============================] - 17s 112ms/step - loss: 0.4325\n",
            "Epoch 19/20\n",
            "149/149 [==============================] - 17s 116ms/step - loss: 0.3922\n",
            "Epoch 20/20\n",
            "149/149 [==============================] - 17s 113ms/step - loss: 0.3537\n",
            "Epoch 1/20\n",
            "133/133 [==============================] - 18s 138ms/step - loss: 2.2377\n",
            "Epoch 2/20\n",
            "133/133 [==============================] - 17s 127ms/step - loss: 1.8781\n",
            "Epoch 3/20\n",
            "133/133 [==============================] - 17s 126ms/step - loss: 1.7486\n",
            "Epoch 4/20\n",
            "133/133 [==============================] - 17s 124ms/step - loss: 1.6282\n",
            "Epoch 5/20\n",
            "133/133 [==============================] - 17s 125ms/step - loss: 1.5122\n",
            "Epoch 6/20\n",
            "133/133 [==============================] - 17s 124ms/step - loss: 1.3804\n",
            "Epoch 7/20\n",
            "133/133 [==============================] - 16s 124ms/step - loss: 1.2505\n",
            "Epoch 8/20\n",
            "133/133 [==============================] - 17s 124ms/step - loss: 1.1257\n",
            "Epoch 9/20\n",
            "133/133 [==============================] - 17s 125ms/step - loss: 1.0118\n",
            "Epoch 10/20\n",
            "133/133 [==============================] - 17s 125ms/step - loss: 0.9122\n",
            "Epoch 11/20\n",
            "133/133 [==============================] - 16s 124ms/step - loss: 0.8242\n",
            "Epoch 12/20\n",
            "133/133 [==============================] - 16s 124ms/step - loss: 0.7449\n",
            "Epoch 13/20\n",
            "133/133 [==============================] - 16s 124ms/step - loss: 0.6749\n",
            "Epoch 14/20\n",
            "133/133 [==============================] - 17s 124ms/step - loss: 0.6096\n",
            "Epoch 15/20\n",
            "133/133 [==============================] - 16s 124ms/step - loss: 0.5489\n",
            "Epoch 16/20\n",
            "133/133 [==============================] - 16s 124ms/step - loss: 0.4920\n",
            "Epoch 17/20\n",
            "133/133 [==============================] - 16s 124ms/step - loss: 0.4393\n",
            "Epoch 18/20\n",
            "133/133 [==============================] - 17s 124ms/step - loss: 0.3890\n",
            "Epoch 19/20\n",
            "133/133 [==============================] - 17s 125ms/step - loss: 0.3443\n",
            "Epoch 20/20\n",
            "133/133 [==============================] - 16s 124ms/step - loss: 0.3033\n",
            "Epoch 1/20\n",
            "119/119 [==============================] - 15s 125ms/step - loss: 2.2728\n",
            "Epoch 2/20\n",
            "119/119 [==============================] - 15s 127ms/step - loss: 1.8936\n",
            "Epoch 3/20\n",
            "119/119 [==============================] - 15s 122ms/step - loss: 1.7785\n",
            "Epoch 4/20\n",
            "119/119 [==============================] - 14s 119ms/step - loss: 1.6692\n",
            "Epoch 5/20\n",
            "119/119 [==============================] - 14s 117ms/step - loss: 1.5592\n",
            "Epoch 6/20\n",
            "119/119 [==============================] - 14s 118ms/step - loss: 1.4457\n",
            "Epoch 7/20\n",
            "119/119 [==============================] - 14s 119ms/step - loss: 1.3310\n",
            "Epoch 8/20\n",
            "119/119 [==============================] - 14s 119ms/step - loss: 1.2171\n",
            "Epoch 9/20\n",
            "119/119 [==============================] - 14s 119ms/step - loss: 1.1077\n",
            "Epoch 10/20\n",
            "119/119 [==============================] - 14s 118ms/step - loss: 1.0089\n",
            "Epoch 11/20\n",
            "119/119 [==============================] - 14s 118ms/step - loss: 0.9204\n",
            "Epoch 12/20\n",
            "119/119 [==============================] - 14s 118ms/step - loss: 0.8431\n",
            "Epoch 13/20\n",
            "119/119 [==============================] - 14s 120ms/step - loss: 0.7738\n",
            "Epoch 14/20\n",
            "119/119 [==============================] - 15s 126ms/step - loss: 0.7107\n",
            "Epoch 15/20\n",
            "119/119 [==============================] - 15s 124ms/step - loss: 0.6518\n",
            "Epoch 16/20\n",
            "119/119 [==============================] - 14s 119ms/step - loss: 0.5973\n",
            "Epoch 17/20\n",
            "119/119 [==============================] - 14s 121ms/step - loss: 0.5460\n",
            "Epoch 18/20\n",
            "119/119 [==============================] - 14s 119ms/step - loss: 0.5000\n",
            "Epoch 19/20\n",
            "119/119 [==============================] - 14s 120ms/step - loss: 0.4552\n",
            "Epoch 20/20\n",
            "119/119 [==============================] - 14s 120ms/step - loss: 0.4131\n",
            "Epoch 1/20\n",
            "109/109 [==============================] - 14s 129ms/step - loss: 2.2801\n",
            "Epoch 2/20\n",
            "109/109 [==============================] - 14s 129ms/step - loss: 1.8815\n",
            "Epoch 3/20\n",
            "109/109 [==============================] - 14s 129ms/step - loss: 1.7607\n",
            "Epoch 4/20\n",
            "109/109 [==============================] - 14s 131ms/step - loss: 1.6540\n",
            "Epoch 5/20\n",
            "109/109 [==============================] - 15s 136ms/step - loss: 1.5484\n",
            "Epoch 6/20\n",
            "109/109 [==============================] - 15s 137ms/step - loss: 1.4398\n",
            "Epoch 7/20\n",
            "109/109 [==============================] - 15s 135ms/step - loss: 1.3264\n",
            "Epoch 8/20\n",
            "109/109 [==============================] - 15s 136ms/step - loss: 1.2121\n",
            "Epoch 9/20\n",
            "109/109 [==============================] - 14s 132ms/step - loss: 1.0993\n",
            "Epoch 10/20\n",
            "109/109 [==============================] - 14s 129ms/step - loss: 0.9936\n",
            "Epoch 11/20\n",
            "109/109 [==============================] - 14s 129ms/step - loss: 0.8998\n",
            "Epoch 12/20\n",
            "109/109 [==============================] - 14s 130ms/step - loss: 0.8132\n",
            "Epoch 13/20\n",
            "109/109 [==============================] - 14s 130ms/step - loss: 0.7363\n",
            "Epoch 14/20\n",
            "109/109 [==============================] - 14s 133ms/step - loss: 0.6655\n",
            "Epoch 15/20\n",
            "109/109 [==============================] - 14s 133ms/step - loss: 0.6009\n",
            "Epoch 16/20\n",
            "109/109 [==============================] - 14s 132ms/step - loss: 0.5390\n",
            "Epoch 17/20\n",
            "109/109 [==============================] - 14s 132ms/step - loss: 0.4820\n",
            "Epoch 18/20\n",
            "109/109 [==============================] - 14s 132ms/step - loss: 0.4295\n",
            "Epoch 19/20\n",
            "109/109 [==============================] - 15s 133ms/step - loss: 0.3816\n",
            "Epoch 20/20\n",
            "109/109 [==============================] - 15s 137ms/step - loss: 0.3379\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXSgW5SdOElp"
      },
      "source": [
        "По результатам видно, что увеличение пространства Embedding уменьшает итоговую ошибку, а также оптимизатор Adam лучше срабатывает, чем RMSprop, теперь подберем размер батча"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD0jbtl0NIPI",
        "outputId": "777be5f7-f25e-4399-ee80-5372495bd3f2"
      },
      "source": [
        "for batch in range(20, 320, 20):\n",
        "  model = make_net(num_filters=500)\n",
        "  model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy')\n",
        "  history = model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=batch, epochs=10) \n",
        "  model.save( f'/content/drive/My Drive/Домашнее задание №26 истории/model_batch_{batch}.h5')\n",
        "  save_hist(history, f'/content/drive/MyDrive/Домашнее задание №26 истории/hist_batch_{batch}.xlsx')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "595/595 [==============================] - 50s 84ms/step - loss: 2.4894\n",
            "Epoch 2/10\n",
            "595/595 [==============================] - 49s 83ms/step - loss: 2.0091\n",
            "Epoch 3/10\n",
            "595/595 [==============================] - 49s 83ms/step - loss: 1.9579\n",
            "Epoch 4/10\n",
            "595/595 [==============================] - 49s 83ms/step - loss: 1.9248\n",
            "Epoch 5/10\n",
            "595/595 [==============================] - 49s 83ms/step - loss: 1.8970\n",
            "Epoch 6/10\n",
            "595/595 [==============================] - 50s 83ms/step - loss: 1.8725\n",
            "Epoch 7/10\n",
            "595/595 [==============================] - 50s 83ms/step - loss: 1.8488\n",
            "Epoch 8/10\n",
            "595/595 [==============================] - 49s 83ms/step - loss: 1.8256\n",
            "Epoch 9/10\n",
            "595/595 [==============================] - 50s 83ms/step - loss: 1.8007\n",
            "Epoch 10/10\n",
            "595/595 [==============================] - 49s 83ms/step - loss: 1.7742\n",
            "Epoch 1/10\n",
            "298/298 [==============================] - 30s 99ms/step - loss: 2.7564\n",
            "Epoch 2/10\n",
            "298/298 [==============================] - 29s 99ms/step - loss: 2.0917\n",
            "Epoch 3/10\n",
            "298/298 [==============================] - 29s 98ms/step - loss: 1.9968\n",
            "Epoch 4/10\n",
            "298/298 [==============================] - 29s 99ms/step - loss: 1.9609\n",
            "Epoch 5/10\n",
            "298/298 [==============================] - 29s 99ms/step - loss: 1.9328\n",
            "Epoch 6/10\n",
            "298/298 [==============================] - 29s 99ms/step - loss: 1.9110\n",
            "Epoch 7/10\n",
            "298/298 [==============================] - 29s 98ms/step - loss: 1.8913\n",
            "Epoch 8/10\n",
            "298/298 [==============================] - 29s 99ms/step - loss: 1.8734\n",
            "Epoch 9/10\n",
            "298/298 [==============================] - 29s 99ms/step - loss: 1.8565\n",
            "Epoch 10/10\n",
            "298/298 [==============================] - 30s 99ms/step - loss: 1.8409\n",
            "Epoch 1/10\n",
            "199/199 [==============================] - 21s 106ms/step - loss: 2.9677\n",
            "Epoch 2/10\n",
            "199/199 [==============================] - 21s 103ms/step - loss: 2.2122\n",
            "Epoch 3/10\n",
            "199/199 [==============================] - 21s 104ms/step - loss: 2.0518\n",
            "Epoch 4/10\n",
            "199/199 [==============================] - 21s 103ms/step - loss: 1.9984\n",
            "Epoch 5/10\n",
            "199/199 [==============================] - 21s 104ms/step - loss: 1.9682\n",
            "Epoch 6/10\n",
            "199/199 [==============================] - 21s 104ms/step - loss: 1.9452\n",
            "Epoch 7/10\n",
            "199/199 [==============================] - 21s 106ms/step - loss: 1.9251\n",
            "Epoch 8/10\n",
            "199/199 [==============================] - 21s 103ms/step - loss: 1.9078\n",
            "Epoch 9/10\n",
            "199/199 [==============================] - 21s 103ms/step - loss: 1.8921\n",
            "Epoch 10/10\n",
            "199/199 [==============================] - 21s 103ms/step - loss: 1.8779\n",
            "Epoch 1/10\n",
            "149/149 [==============================] - 18s 118ms/step - loss: 3.1390\n",
            "Epoch 2/10\n",
            "149/149 [==============================] - 18s 118ms/step - loss: 2.2883\n",
            "Epoch 3/10\n",
            "149/149 [==============================] - 17s 117ms/step - loss: 2.1372\n",
            "Epoch 4/10\n",
            "149/149 [==============================] - 18s 118ms/step - loss: 2.0388\n",
            "Epoch 5/10\n",
            "149/149 [==============================] - 18s 121ms/step - loss: 1.9996\n",
            "Epoch 6/10\n",
            "149/149 [==============================] - 18s 118ms/step - loss: 1.9749\n",
            "Epoch 7/10\n",
            "149/149 [==============================] - 17s 117ms/step - loss: 1.9539\n",
            "Epoch 8/10\n",
            "149/149 [==============================] - 17s 117ms/step - loss: 1.9357\n",
            "Epoch 9/10\n",
            "149/149 [==============================] - 17s 117ms/step - loss: 1.9196\n",
            "Epoch 10/10\n",
            "149/149 [==============================] - 18s 118ms/step - loss: 1.9050\n",
            "Epoch 1/10\n",
            "119/119 [==============================] - 15s 126ms/step - loss: 3.3273\n",
            "Epoch 2/10\n",
            "119/119 [==============================] - 15s 126ms/step - loss: 2.3215\n",
            "Epoch 3/10\n",
            "119/119 [==============================] - 15s 125ms/step - loss: 2.2001\n",
            "Epoch 4/10\n",
            "119/119 [==============================] - 15s 130ms/step - loss: 2.0901\n",
            "Epoch 5/10\n",
            "119/119 [==============================] - 15s 125ms/step - loss: 2.0284\n",
            "Epoch 6/10\n",
            "119/119 [==============================] - 15s 123ms/step - loss: 1.9987\n",
            "Epoch 7/10\n",
            "119/119 [==============================] - 15s 124ms/step - loss: 1.9766\n",
            "Epoch 8/10\n",
            "119/119 [==============================] - 15s 124ms/step - loss: 1.9574\n",
            "Epoch 9/10\n",
            "119/119 [==============================] - 15s 124ms/step - loss: 1.9404\n",
            "Epoch 10/10\n",
            "119/119 [==============================] - 15s 125ms/step - loss: 1.9249\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 13s 133ms/step - loss: 3.4988\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 13s 134ms/step - loss: 2.3531\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 13s 134ms/step - loss: 2.2497\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 13s 133ms/step - loss: 2.1630\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 14s 137ms/step - loss: 2.0751\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 14s 136ms/step - loss: 2.0259\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 13s 133ms/step - loss: 2.0006\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 13s 133ms/step - loss: 1.9820\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 13s 133ms/step - loss: 1.9652\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 13s 133ms/step - loss: 1.9493\n",
            "Epoch 1/10\n",
            "85/85 [==============================] - 12s 140ms/step - loss: 3.6699\n",
            "Epoch 2/10\n",
            "85/85 [==============================] - 12s 139ms/step - loss: 2.3809\n",
            "Epoch 3/10\n",
            "85/85 [==============================] - 12s 139ms/step - loss: 2.2672\n",
            "Epoch 4/10\n",
            "85/85 [==============================] - 12s 139ms/step - loss: 2.1987\n",
            "Epoch 5/10\n",
            "85/85 [==============================] - 12s 138ms/step - loss: 2.1233\n",
            "Epoch 6/10\n",
            "85/85 [==============================] - 12s 139ms/step - loss: 2.0575\n",
            "Epoch 7/10\n",
            "85/85 [==============================] - 12s 139ms/step - loss: 2.0208\n",
            "Epoch 8/10\n",
            "85/85 [==============================] - 12s 138ms/step - loss: 1.9982\n",
            "Epoch 9/10\n",
            "85/85 [==============================] - 12s 138ms/step - loss: 1.9804\n",
            "Epoch 10/10\n",
            "85/85 [==============================] - 12s 143ms/step - loss: 1.9649\n",
            "Epoch 1/10\n",
            "75/75 [==============================] - 12s 159ms/step - loss: 3.7994\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 12s 158ms/step - loss: 2.4303\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 12s 157ms/step - loss: 2.2915\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 12s 157ms/step - loss: 2.2370\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 12s 156ms/step - loss: 2.1827\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 12s 156ms/step - loss: 2.1178\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 12s 156ms/step - loss: 2.0600\n",
            "Epoch 8/10\n",
            "75/75 [==============================] - 12s 155ms/step - loss: 2.0262\n",
            "Epoch 9/10\n",
            "75/75 [==============================] - 12s 156ms/step - loss: 2.0059\n",
            "Epoch 10/10\n",
            "75/75 [==============================] - 12s 156ms/step - loss: 1.9905\n",
            "Epoch 1/10\n",
            "67/67 [==============================] - 11s 160ms/step - loss: 3.9039\n",
            "Epoch 2/10\n",
            "67/67 [==============================] - 11s 163ms/step - loss: 2.5246\n",
            "Epoch 3/10\n",
            "67/67 [==============================] - 11s 167ms/step - loss: 2.3085\n",
            "Epoch 4/10\n",
            "67/67 [==============================] - 11s 160ms/step - loss: 2.2559\n",
            "Epoch 5/10\n",
            "67/67 [==============================] - 11s 162ms/step - loss: 2.2103\n",
            "Epoch 6/10\n",
            "67/67 [==============================] - 11s 159ms/step - loss: 2.1559\n",
            "Epoch 7/10\n",
            "67/67 [==============================] - 11s 161ms/step - loss: 2.0947\n",
            "Epoch 8/10\n",
            "67/67 [==============================] - 11s 160ms/step - loss: 2.0485\n",
            "Epoch 9/10\n",
            "67/67 [==============================] - 11s 163ms/step - loss: 2.0213\n",
            "Epoch 10/10\n",
            "67/67 [==============================] - 11s 160ms/step - loss: 2.0037\n",
            "Epoch 1/10\n",
            "60/60 [==============================] - 11s 183ms/step - loss: 3.9493\n",
            "Epoch 2/10\n",
            "60/60 [==============================] - 10s 174ms/step - loss: 2.6424\n",
            "Epoch 3/10\n",
            "60/60 [==============================] - 10s 175ms/step - loss: 2.3197\n",
            "Epoch 4/10\n",
            "60/60 [==============================] - 10s 174ms/step - loss: 2.2661\n",
            "Epoch 5/10\n",
            "60/60 [==============================] - 10s 174ms/step - loss: 2.2266\n",
            "Epoch 6/10\n",
            "60/60 [==============================] - 10s 173ms/step - loss: 2.1858\n",
            "Epoch 7/10\n",
            "60/60 [==============================] - 11s 175ms/step - loss: 2.1368\n",
            "Epoch 8/10\n",
            "60/60 [==============================] - 10s 174ms/step - loss: 2.0832\n",
            "Epoch 9/10\n",
            "60/60 [==============================] - 10s 174ms/step - loss: 2.0430\n",
            "Epoch 10/10\n",
            "60/60 [==============================] - 10s 174ms/step - loss: 2.0192\n",
            "Epoch 1/10\n",
            "55/55 [==============================] - 10s 181ms/step - loss: 3.9779\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - 10s 178ms/step - loss: 2.7904\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - 10s 178ms/step - loss: 2.3303\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - 10s 178ms/step - loss: 2.2724\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - 10s 177ms/step - loss: 2.2343\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - 10s 177ms/step - loss: 2.2001\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - 10s 176ms/step - loss: 2.1617\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - 10s 178ms/step - loss: 2.1142\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - 10s 178ms/step - loss: 2.0695\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - 10s 176ms/step - loss: 2.0368\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 10s 192ms/step - loss: 3.9913\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 9s 188ms/step - loss: 2.9683\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 9s 190ms/step - loss: 2.3450\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 9s 189ms/step - loss: 2.2813\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 9s 188ms/step - loss: 2.2429\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 9s 189ms/step - loss: 2.2111\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 10s 192ms/step - loss: 2.1790\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 9s 189ms/step - loss: 2.1410\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 9s 190ms/step - loss: 2.0978\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 10s 191ms/step - loss: 2.0605\n",
            "Epoch 1/10\n",
            "46/46 [==============================] - 9s 204ms/step - loss: 3.9972\n",
            "Epoch 2/10\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 3.1260\n",
            "Epoch 3/10\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 2.3628\n",
            "Epoch 4/10\n",
            "46/46 [==============================] - 9s 203ms/step - loss: 2.2899\n",
            "Epoch 5/10\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 2.2532\n",
            "Epoch 6/10\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 2.2242\n",
            "Epoch 7/10\n",
            "46/46 [==============================] - 9s 202ms/step - loss: 2.1973\n",
            "Epoch 8/10\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 2.1681\n",
            "Epoch 9/10\n",
            "46/46 [==============================] - 9s 201ms/step - loss: 2.1323\n",
            "Epoch 10/10\n",
            "46/46 [==============================] - 9s 199ms/step - loss: 2.0929\n",
            "Epoch 1/10\n",
            "43/43 [==============================] - 9s 218ms/step - loss: 4.0013\n",
            "Epoch 2/10\n",
            "43/43 [==============================] - 9s 217ms/step - loss: 3.2793\n",
            "Epoch 3/10\n",
            "43/43 [==============================] - 9s 219ms/step - loss: 2.3858\n",
            "Epoch 4/10\n",
            "43/43 [==============================] - 9s 216ms/step - loss: 2.3042\n",
            "Epoch 5/10\n",
            "43/43 [==============================] - 10s 221ms/step - loss: 2.2671\n",
            "Epoch 6/10\n",
            "43/43 [==============================] - 10s 229ms/step - loss: 2.2390\n",
            "Epoch 7/10\n",
            "43/43 [==============================] - 10s 228ms/step - loss: 2.2144\n",
            "Epoch 8/10\n",
            "43/43 [==============================] - 9s 217ms/step - loss: 2.1891\n",
            "Epoch 9/10\n",
            "43/43 [==============================] - 9s 217ms/step - loss: 2.1588\n",
            "Epoch 10/10\n",
            "43/43 [==============================] - 9s 214ms/step - loss: 2.1234\n",
            "Epoch 1/10\n",
            "40/40 [==============================] - 9s 227ms/step - loss: 4.0032\n",
            "Epoch 2/10\n",
            "40/40 [==============================] - 9s 227ms/step - loss: 3.4120\n",
            "Epoch 3/10\n",
            "40/40 [==============================] - 9s 226ms/step - loss: 2.4179\n",
            "Epoch 4/10\n",
            "40/40 [==============================] - 9s 227ms/step - loss: 2.3179\n",
            "Epoch 5/10\n",
            "40/40 [==============================] - 9s 231ms/step - loss: 2.2788\n",
            "Epoch 6/10\n",
            "40/40 [==============================] - 9s 227ms/step - loss: 2.2524\n",
            "Epoch 7/10\n",
            "40/40 [==============================] - 9s 226ms/step - loss: 2.2288\n",
            "Epoch 8/10\n",
            "40/40 [==============================] - 9s 227ms/step - loss: 2.2057\n",
            "Epoch 9/10\n",
            "40/40 [==============================] - 9s 227ms/step - loss: 2.1820\n",
            "Epoch 10/10\n",
            "40/40 [==============================] - 9s 227ms/step - loss: 2.1536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMhPAuMW8n3n"
      },
      "source": [
        "all_data = pd.DataFrame()\n",
        "for batch in range(20, 320, 20):\n",
        "  df = pd.read_excel(f'/content/drive/MyDrive/Домашнее задание №26 истории/hist_batch_{batch}.xlsx')\n",
        "  all_data[f'{batch}'] = df['loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It3_7S9Z8n1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "0d7fa334-9d25-4ab8-d2b6-4f7dbab3814b"
      },
      "source": [
        "all_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>20</th>\n",
              "      <th>40</th>\n",
              "      <th>60</th>\n",
              "      <th>80</th>\n",
              "      <th>100</th>\n",
              "      <th>120</th>\n",
              "      <th>140</th>\n",
              "      <th>160</th>\n",
              "      <th>180</th>\n",
              "      <th>200</th>\n",
              "      <th>220</th>\n",
              "      <th>240</th>\n",
              "      <th>260</th>\n",
              "      <th>280</th>\n",
              "      <th>300</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.489378</td>\n",
              "      <td>2.756364</td>\n",
              "      <td>2.967692</td>\n",
              "      <td>3.138956</td>\n",
              "      <td>3.327331</td>\n",
              "      <td>3.498780</td>\n",
              "      <td>3.669918</td>\n",
              "      <td>3.799428</td>\n",
              "      <td>3.903911</td>\n",
              "      <td>3.949324</td>\n",
              "      <td>3.977857</td>\n",
              "      <td>3.991331</td>\n",
              "      <td>3.997227</td>\n",
              "      <td>4.001312</td>\n",
              "      <td>4.003178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.009057</td>\n",
              "      <td>2.091726</td>\n",
              "      <td>2.212188</td>\n",
              "      <td>2.288288</td>\n",
              "      <td>2.321468</td>\n",
              "      <td>2.353065</td>\n",
              "      <td>2.380901</td>\n",
              "      <td>2.430278</td>\n",
              "      <td>2.524612</td>\n",
              "      <td>2.642445</td>\n",
              "      <td>2.790395</td>\n",
              "      <td>2.968255</td>\n",
              "      <td>3.126038</td>\n",
              "      <td>3.279263</td>\n",
              "      <td>3.412049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.957857</td>\n",
              "      <td>1.996805</td>\n",
              "      <td>2.051809</td>\n",
              "      <td>2.137204</td>\n",
              "      <td>2.200061</td>\n",
              "      <td>2.249652</td>\n",
              "      <td>2.267210</td>\n",
              "      <td>2.291483</td>\n",
              "      <td>2.308522</td>\n",
              "      <td>2.319743</td>\n",
              "      <td>2.330280</td>\n",
              "      <td>2.344987</td>\n",
              "      <td>2.362784</td>\n",
              "      <td>2.385781</td>\n",
              "      <td>2.417864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.924838</td>\n",
              "      <td>1.960872</td>\n",
              "      <td>1.998377</td>\n",
              "      <td>2.038769</td>\n",
              "      <td>2.090084</td>\n",
              "      <td>2.163049</td>\n",
              "      <td>2.198693</td>\n",
              "      <td>2.236951</td>\n",
              "      <td>2.255876</td>\n",
              "      <td>2.266106</td>\n",
              "      <td>2.272356</td>\n",
              "      <td>2.281325</td>\n",
              "      <td>2.289937</td>\n",
              "      <td>2.304222</td>\n",
              "      <td>2.317871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.896997</td>\n",
              "      <td>1.932847</td>\n",
              "      <td>1.968169</td>\n",
              "      <td>1.999640</td>\n",
              "      <td>2.028443</td>\n",
              "      <td>2.075102</td>\n",
              "      <td>2.123301</td>\n",
              "      <td>2.182661</td>\n",
              "      <td>2.210331</td>\n",
              "      <td>2.226598</td>\n",
              "      <td>2.234301</td>\n",
              "      <td>2.242934</td>\n",
              "      <td>2.253206</td>\n",
              "      <td>2.267094</td>\n",
              "      <td>2.278799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.872499</td>\n",
              "      <td>1.910995</td>\n",
              "      <td>1.945164</td>\n",
              "      <td>1.974864</td>\n",
              "      <td>1.998705</td>\n",
              "      <td>2.025887</td>\n",
              "      <td>2.057469</td>\n",
              "      <td>2.117846</td>\n",
              "      <td>2.155864</td>\n",
              "      <td>2.185797</td>\n",
              "      <td>2.200113</td>\n",
              "      <td>2.211080</td>\n",
              "      <td>2.224158</td>\n",
              "      <td>2.238998</td>\n",
              "      <td>2.252420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.848825</td>\n",
              "      <td>1.891250</td>\n",
              "      <td>1.925139</td>\n",
              "      <td>1.953912</td>\n",
              "      <td>1.976635</td>\n",
              "      <td>2.000648</td>\n",
              "      <td>2.020827</td>\n",
              "      <td>2.060033</td>\n",
              "      <td>2.094675</td>\n",
              "      <td>2.136755</td>\n",
              "      <td>2.161730</td>\n",
              "      <td>2.178980</td>\n",
              "      <td>2.197343</td>\n",
              "      <td>2.214417</td>\n",
              "      <td>2.228781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.825592</td>\n",
              "      <td>1.873355</td>\n",
              "      <td>1.907804</td>\n",
              "      <td>1.935656</td>\n",
              "      <td>1.957372</td>\n",
              "      <td>1.981958</td>\n",
              "      <td>1.998245</td>\n",
              "      <td>2.026249</td>\n",
              "      <td>2.048469</td>\n",
              "      <td>2.083228</td>\n",
              "      <td>2.114176</td>\n",
              "      <td>2.140981</td>\n",
              "      <td>2.168073</td>\n",
              "      <td>2.189061</td>\n",
              "      <td>2.205723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.800675</td>\n",
              "      <td>1.856472</td>\n",
              "      <td>1.892063</td>\n",
              "      <td>1.919605</td>\n",
              "      <td>1.940367</td>\n",
              "      <td>1.965160</td>\n",
              "      <td>1.980411</td>\n",
              "      <td>2.005950</td>\n",
              "      <td>2.021254</td>\n",
              "      <td>2.042954</td>\n",
              "      <td>2.069511</td>\n",
              "      <td>2.097799</td>\n",
              "      <td>2.132324</td>\n",
              "      <td>2.158781</td>\n",
              "      <td>2.181984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.774227</td>\n",
              "      <td>1.840930</td>\n",
              "      <td>1.877867</td>\n",
              "      <td>1.904970</td>\n",
              "      <td>1.924895</td>\n",
              "      <td>1.949285</td>\n",
              "      <td>1.964891</td>\n",
              "      <td>1.990484</td>\n",
              "      <td>2.003653</td>\n",
              "      <td>2.019176</td>\n",
              "      <td>2.036813</td>\n",
              "      <td>2.060467</td>\n",
              "      <td>2.092930</td>\n",
              "      <td>2.123384</td>\n",
              "      <td>2.153593</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         20        40        60  ...       260       280       300\n",
              "0  2.489378  2.756364  2.967692  ...  3.997227  4.001312  4.003178\n",
              "1  2.009057  2.091726  2.212188  ...  3.126038  3.279263  3.412049\n",
              "2  1.957857  1.996805  2.051809  ...  2.362784  2.385781  2.417864\n",
              "3  1.924838  1.960872  1.998377  ...  2.289937  2.304222  2.317871\n",
              "4  1.896997  1.932847  1.968169  ...  2.253206  2.267094  2.278799\n",
              "5  1.872499  1.910995  1.945164  ...  2.224158  2.238998  2.252420\n",
              "6  1.848825  1.891250  1.925139  ...  2.197343  2.214417  2.228781\n",
              "7  1.825592  1.873355  1.907804  ...  2.168073  2.189061  2.205723\n",
              "8  1.800675  1.856472  1.892063  ...  2.132324  2.158781  2.181984\n",
              "9  1.774227  1.840930  1.877867  ...  2.092930  2.123384  2.153593\n",
              "\n",
              "[10 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIEc4iGadN80"
      },
      "source": [
        "Чем меньше батч, тем меньше ошибка. Соберем наши гиперпараметры в одну сеть, \n",
        "батч = 16, оптимизатор = Адам, пространство ембединга = 500 и поставим обучени на 150 эпох"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqojK8ef8nzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e3948b-facb-4232-a55c-10d664f0b2a0"
      },
      "source": [
        "model = make_net(num_filters=500)\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy')\n",
        "history = model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=16, epochs=150) \n",
        "model.save( f'/content/drive/My Drive/Домашнее задание №26 истории/model_best.h5')\n",
        "save_hist(history, f'/content/drive/MyDrive/Домашнее задание №26 истории/hist_best.xlsx')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 2.4283\n",
            "Epoch 2/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.9982\n",
            "Epoch 3/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.9506\n",
            "Epoch 4/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 1.9167\n",
            "Epoch 5/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.8870\n",
            "Epoch 6/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 1.8588\n",
            "Epoch 7/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.8299\n",
            "Epoch 8/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 1.7993\n",
            "Epoch 9/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 1.7677\n",
            "Epoch 10/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 1.7362\n",
            "Epoch 11/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.7052\n",
            "Epoch 12/150\n",
            "744/744 [==============================] - 64s 87ms/step - loss: 1.6741\n",
            "Epoch 13/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.6436\n",
            "Epoch 14/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.6122\n",
            "Epoch 15/150\n",
            "744/744 [==============================] - 64s 86ms/step - loss: 1.5814\n",
            "Epoch 16/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.5515\n",
            "Epoch 17/150\n",
            "744/744 [==============================] - 64s 86ms/step - loss: 1.5216\n",
            "Epoch 18/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.4918\n",
            "Epoch 19/150\n",
            "744/744 [==============================] - 64s 86ms/step - loss: 1.4626\n",
            "Epoch 20/150\n",
            "744/744 [==============================] - 64s 86ms/step - loss: 1.4335\n",
            "Epoch 21/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.4050\n",
            "Epoch 22/150\n",
            "744/744 [==============================] - 64s 86ms/step - loss: 1.3766\n",
            "Epoch 23/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.3482\n",
            "Epoch 24/150\n",
            "744/744 [==============================] - 64s 86ms/step - loss: 1.3199\n",
            "Epoch 25/150\n",
            "744/744 [==============================] - 64s 86ms/step - loss: 1.2919\n",
            "Epoch 26/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 1.2643\n",
            "Epoch 27/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 1.2363\n",
            "Epoch 28/150\n",
            "744/744 [==============================] - 67s 90ms/step - loss: 1.2093\n",
            "Epoch 29/150\n",
            "744/744 [==============================] - 66s 89ms/step - loss: 1.1814\n",
            "Epoch 30/150\n",
            "744/744 [==============================] - 67s 89ms/step - loss: 1.1548\n",
            "Epoch 31/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 1.1281\n",
            "Epoch 32/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 1.1019\n",
            "Epoch 33/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 1.0769\n",
            "Epoch 34/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.0512\n",
            "Epoch 35/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 1.0261\n",
            "Epoch 36/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 1.0019\n",
            "Epoch 37/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.9785\n",
            "Epoch 38/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.9555\n",
            "Epoch 39/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.9321\n",
            "Epoch 40/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.9103\n",
            "Epoch 41/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.8881\n",
            "Epoch 42/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.8673\n",
            "Epoch 43/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.8464\n",
            "Epoch 44/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.8260\n",
            "Epoch 45/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.8056\n",
            "Epoch 46/150\n",
            "744/744 [==============================] - 64s 87ms/step - loss: 0.7857\n",
            "Epoch 47/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.7662\n",
            "Epoch 48/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.7472\n",
            "Epoch 49/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.7289\n",
            "Epoch 50/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.7108\n",
            "Epoch 51/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.6921\n",
            "Epoch 52/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.6740\n",
            "Epoch 53/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.6567\n",
            "Epoch 54/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.6391\n",
            "Epoch 55/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.6220\n",
            "Epoch 56/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.6048\n",
            "Epoch 57/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.5883\n",
            "Epoch 58/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.5717\n",
            "Epoch 59/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.5560\n",
            "Epoch 60/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.5404\n",
            "Epoch 61/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.5241\n",
            "Epoch 62/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.5089\n",
            "Epoch 63/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.4941\n",
            "Epoch 64/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.4795\n",
            "Epoch 65/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.4649\n",
            "Epoch 66/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.4505\n",
            "Epoch 67/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.4358\n",
            "Epoch 68/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.4218\n",
            "Epoch 69/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.4089\n",
            "Epoch 70/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.3953\n",
            "Epoch 71/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.3822\n",
            "Epoch 72/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.3689\n",
            "Epoch 73/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.3566\n",
            "Epoch 74/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.3451\n",
            "Epoch 75/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.3333\n",
            "Epoch 76/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.3203\n",
            "Epoch 77/150\n",
            "744/744 [==============================] - 65s 87ms/step - loss: 0.3087\n",
            "Epoch 78/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.2975\n",
            "Epoch 79/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.2866\n",
            "Epoch 80/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.2764\n",
            "Epoch 81/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.2664\n",
            "Epoch 82/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.2558\n",
            "Epoch 83/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.2455\n",
            "Epoch 84/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.2364\n",
            "Epoch 85/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.2272\n",
            "Epoch 86/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.2186\n",
            "Epoch 87/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.2097\n",
            "Epoch 88/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.2022\n",
            "Epoch 89/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.1930\n",
            "Epoch 90/150\n",
            "744/744 [==============================] - 66s 89ms/step - loss: 0.1845\n",
            "Epoch 91/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.1776\n",
            "Epoch 92/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.1706\n",
            "Epoch 93/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.1634\n",
            "Epoch 94/150\n",
            "744/744 [==============================] - 66s 89ms/step - loss: 0.1567\n",
            "Epoch 95/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.1506\n",
            "Epoch 96/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.1439\n",
            "Epoch 97/150\n",
            "744/744 [==============================] - 65s 88ms/step - loss: 0.1375\n",
            "Epoch 98/150\n",
            "744/744 [==============================] - 66s 89ms/step - loss: 0.1319\n",
            "Epoch 99/150\n",
            "744/744 [==============================] - 66s 89ms/step - loss: 0.1267\n",
            "Epoch 100/150\n",
            "744/744 [==============================] - 66s 88ms/step - loss: 0.1214\n",
            "Epoch 101/150\n",
            "744/744 [==============================] - 66s 89ms/step - loss: 0.1166\n",
            "Epoch 102/150\n",
            "744/744 [==============================] - 66s 89ms/step - loss: 0.1119\n",
            "Epoch 103/150\n",
            "744/744 [==============================] - 66s 89ms/step - loss: 0.1071\n",
            "Epoch 104/150\n",
            "744/744 [==============================] - 67s 89ms/step - loss: 0.1025\n",
            "Epoch 105/150\n",
            "157/744 [=====>........................] - ETA: 52s - loss: 0.0837"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}